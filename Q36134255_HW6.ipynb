{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwannn666/GAN_AI/blob/main/Q36134255_HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okd5lMH5FGsw"
      },
      "source": [
        "# **正反方辯論機器人**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2ZaP8HOa2-G"
      },
      "source": [
        "[Ollama](https://ollama.com/) 可以讓我們在自己的機器上跑開源的大型語言模型, 並且用 API 的方式呼叫。這裡我們介紹在 Colab 上跑, 並且分別用 OpenAI 的 API, 及 [`aisuite` 套件](https://github.com/andrewyng/aisuite) 來使用 Ollama 提供的大型語言模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_m1Lr5Vbi7x"
      },
      "source": [
        "### 1. 安裝並執行 Ollama\n",
        "\n",
        "首先是到官網抓下安裝程式, 並且安裝。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYy1Nb7hH6Za",
        "outputId": "0db29db9-d6e6-4f6a-eb49-3e9b6ba48fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1V4Jx9IFARK"
      },
      "source": [
        "讀入標準套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQmm4kJfEz4s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz0sXeelbtrL"
      },
      "source": [
        "因為是使用 API 的方式呼叫, 所以要運行 Ollama Server, 而這邊是放在背景執行。\n",
        "補充 : 如果在執行程式時有斷線的情況發生需要重新執行此程式碼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8522mlhhQjFF",
        "outputId": "e261d80d-31f2-4b00-f0ef-4d7233853789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ],
      "source": [
        "!nohup ollama serve &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH1gAWqgb6lJ"
      },
      "source": [
        "這邊我使用的是在 ollama 的兩個不同模型，分別為 gemma3:4b 跟 mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RACuI7baNspA",
        "outputId": "e399da8e-db59-4128-aae7-aeee555406d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling aeda25e63ebd... 100% ▕▏ 3.3 GB                         \u001b[K\n",
            "pulling e0a42594d802... 100% ▕▏  358 B                         \u001b[K\n",
            "pulling dd084c7d92a3... 100% ▕▏ 8.4 KB                         \u001b[K\n",
            "pulling 3116c5225075... 100% ▕▏   77 B                         \u001b[K\n",
            "pulling b6ae5839783f... 100% ▕▏  489 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "# 拉取模型 A：gemma3:4b\n",
        "!ollama pull gemma3:4b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQtmZPGoyuwz",
        "outputId": "2fcb8526-4dba-43f5-8d0c-7808863fe250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling ff82381e2bea... 100% ▕▏ 4.1 GB                         \u001b[K\n",
            "pulling 43070e2d4e53... 100% ▕▏  11 KB                         \u001b[K\n",
            "pulling 491dfa501e59... 100% ▕▏  801 B                         \u001b[K\n",
            "pulling ed11eda7790d... 100% ▕▏   30 B                         \u001b[K\n",
            "pulling 42347cd80dc8... 100% ▕▏  485 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "# 拉取模型 B：mistral\n",
        "!ollama pull mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZXJhweHcLTu"
      },
      "source": [
        "### 2. 用 OpenAI API 使用\n",
        "\n",
        "因為 ChatGPT 大概是最早紅的大型語言模型, 因此許多大型語言模型, 都和 OpenAI API 相容, Ollama 也不例外。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYOt-79JSuAC"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA0BCHUdcdKb"
      },
      "source": [
        "本來是需要 OpenAI 金鑰, 但我們沒有真的要用 OpenAI 的服務, 金鑰就亂打一通就好。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMt3IPnOIZ40"
      },
      "outputs": [],
      "source": [
        "api_key = \"ollama\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tb7nDuqcq0Y"
      },
      "source": [
        " OpenAI API 打開 `client` 的方式 , 預設服務 `port` 是 `11434`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLAJsGC5SpX4"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"http://localhost:11434/v1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ci3D3sKaAwO"
      },
      "source": [
        "### 3.正反方辯論機器人模型架構\n",
        "\n",
        "記得角色 (role) 一共有三種, 分別是:\n",
        "\n",
        "* `systemA`: 這是正方對話機器人的「人設」\n",
        "* `systemB`: 這是反方對話機器人的「人設」\n",
        "* `user`: 使用者\n",
        "* `assistant`: ChatGPT 的回應"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_pEQHf22GrN"
      },
      "outputs": [],
      "source": [
        "model_A = \"gemma3:4b\"    # 正方\n",
        "model_B = \"mistral\"     # 反方"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cb3PWCU-ZKK"
      },
      "source": [
        "正方對話機器人的「人設」:\n",
        "\n",
        "\"你是一位邏輯嚴謹、辯才無礙的正方辯論者，擅長站在議題的支持角度提出實證與理性觀點。請使用有說服力的語氣，避免太過冗長，一次只發表一個觀點,儘量不要超過二十個字。請用台灣習慣的中文來回應。\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRTa6E8nay4b"
      },
      "outputs": [],
      "source": [
        "system_A = \"你是一位邏輯嚴謹、辯才無礙的正方辯論者，擅長站在議題的支持角度提出實證與理性觀點。請使用有說服力的語氣，避免太過冗長，一次只發表一個觀點,儘量不要超過二十個字。請用台灣習慣的中文來回應。\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4PY5AqT-haB"
      },
      "source": [
        "正方對話機器人的「人設」:\n",
        "\n",
        "\"你是一位尖銳反思、觀點犀利的反方辯論者，擅長質疑主流觀點並從反面切入議題。請使用有力的批判性語氣，每次一句論述即可，避免過長,儘量不要超過二十個字。請用台灣習慣的中文來回應。\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7TBb-gA1vgE"
      },
      "outputs": [],
      "source": [
        "system_B = \"你是一位尖銳反思、觀點犀利的反方辯論者，擅長質疑主流觀點並從反面切入議題。請使用有力的批判性語氣，每次一句論述即可，避免過長,儘量不要超過二十個字。請用台灣習慣的中文來回應。\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQiAlShV9hxS"
      },
      "outputs": [],
      "source": [
        "def init_debate_messages(user_prompt):\n",
        "    messages_A = [\n",
        "        {\"role\": \"system\", \"content\": system_A},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    messages_B = [\n",
        "        {\"role\": \"system\", \"content\": system_B},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    return messages_A, messages_B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSEqb6dr-oRd"
      },
      "source": [
        "這邊我自訂了輸入，可以根據想讓兩個不同模型的對話機器人進行辯論的議題進行輸入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWCRpErd9j45",
        "outputId": "227fb2dc-5a3f-4766-b644-e8c123922edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "請輸入辯論主題：先有蛋還是先有雞?\n"
          ]
        }
      ],
      "source": [
        "prompt = input(\"請輸入辯論主題：\")\n",
        "messages_A, messages_B = init_debate_messages(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-Ut9TmF-8GL"
      },
      "source": [
        "這邊先讓正方發言，並且使用 `reply_A` 來擷取回應 ，並使用 `append` 分別讓正反方更新對話歷史"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R4XS2GIbnfe",
        "outputId": "919f8c8c-9d9b-43d5-c2fc-6d3318d39d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正方： 科學證據顯示：先有蛋，並非基因突變的過程。\n"
          ]
        }
      ],
      "source": [
        "response_A = client.chat.completions.create(\n",
        "    model=model_A,\n",
        "    messages=messages_A\n",
        ")\n",
        "\n",
        "reply_A = response_A.choices[0].message.content.strip()\n",
        "print(\"正方：\", reply_A)\n",
        "\n",
        "\n",
        "messages_A.append({\"role\": \"assistant\", \"content\": reply_A})\n",
        "messages_B.append({\"role\": \"user\", \"content\": reply_A})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI-yY9Fs_ggv"
      },
      "source": [
        "接著讓反方發言，並且使用 `reply_A` 來擷取回應 ，並同樣使用 `append` 分別讓正反方更新對話歷史"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP0JAZJW3bN0",
        "outputId": "6ba52948-1bab-4529-c79e-3e0e937a8b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "反方： 先有蛋，進而生雞，更接近實際、可測量。基因突變？只是論述中的幻想故事。\n"
          ]
        }
      ],
      "source": [
        "response_B = client.chat.completions.create(\n",
        "    model=model_B,\n",
        "    messages=messages_B\n",
        ")\n",
        "\n",
        "reply_B = response_B.choices[0].message.content.strip()\n",
        "print(\"反方：\", reply_B)\n",
        "\n",
        "messages_B.append({\"role\": \"assistant\", \"content\": reply_B})\n",
        "messages_A.append({\"role\": \"user\", \"content\": reply_B})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIR7Z9Ck_5M_"
      },
      "source": [
        "使用 for 迴圈讓對話進行 9 次 並且讓正反方互相展開辯論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4cFn0y5d3erO",
        "outputId": "fd78086c-a1c2-4166-9c47-11982c2232d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "第 2 回合\n",
            "反方： 你認為文化傳統之間哪些方面需要在全球化中重新思考？\n",
            "\n",
            "文化各有千秋，在全球化過程中，需要回顧並重新思考以下三個方面：\n",
            "\n",
            "1. 議題定位：將不同的文化問題定義成條件下通用化的標準，其實是忽略相關基礎情況、卻造成錯誤價值評価。在全球化中，我們應該更多使用對等、尊重差異的方式來回顧文化問題，並定位出相關問題空間。\n",
            "2. 信息溝通：不同的文化背景造成了不同的信息溝通方式和挑戰，在全球化中要求我們更好地理解不同的文化差異，並學習更好地透過相關交流廣域來討論並處理問題。\n",
            "3. 教育模式改革：在全球化中，我們需要將各種文化的知識和經驗融合來擁有更全面的認知和能力，而不是堅持單一文化的教育模式。此外，在全球化中，學習語言、文化相關的能力也是非常重要的，可以更好地理解和互動不同文化的人群。\n",
            "正方： 尊重多元文化本體，而非刻意定義標準與價值。\n",
            "\n",
            "第 3 回合\n",
            "反方： 標準與價值不是一一對等的，文化區別可以讓我們更好地相互理解與沟通。\n",
            "正方： 差異是溝通資本，相容性是新時代的基石。\n",
            "\n",
            "第 4 回合\n",
            "反方： 危險化差異，忽略兼容性，不足為今。\n",
            "正方： 包容多元，合作共贏，務實至上。\n",
            "\n",
            "第 5 回合\n",
            "反方： 包容性是不會提升人類之力量。合作共利需求自本心出發。「務實」是無情的嗜金富玉。\n",
            "正方： 理性與意志並行，平衡是永恆命題。\n",
            "\n",
            "第 6 回合\n",
            "反方： 「有時候，意志驅使人向前進步，理性則是制約之力，否則進步可能無法控制。」\n",
            "正方： 平衡力量與理性，創造更穩健未來。\n",
            "\n",
            "第 7 回合\n",
            "反方： 在實際生活中，只能先做事再去求義，不然就無法建立基石，進而發展。反之亦然，如果據以義求實，難免過度讓理論和想象影響現實，造成混亂和違例。為了創造穩健的未來，實際生活是當前最重要的基石，而不是平衡力量與理性。\n",
            "正方： 實踐至上，基業永續，務實是根本。\n",
            "\n",
            "第 8 回合\n",
            "反方： 在實際行動時，難免對現成價值產出影響。「基業永續」，無法完全隔離於實際，必須賦予其相互關係。如果只重視根本的「務實」，未來將無從補足。\n",
            "正方： 穩健發展，層層遞進，基石即是提升。\n",
            "\n",
            "第 9 回合\n",
            "反方： 「強化主流觀點的維持，阻礙真正的進步。」\n",
            "正方： 突破固有框架，創新思維，進步永無止境。\n",
            "\n",
            "第 10 回合\n",
            "反方： 在斷舊建新時，最多只能保留一小部分基石，不必對過往產出情感。\n",
            "正方： 理性拆除過往，開創嶄新局面決定。\n"
          ]
        }
      ],
      "source": [
        "# 進行後續 9 輪\n",
        "for i in range(9):\n",
        "    print(f\"\\n第 {i+2} 回合\")\n",
        "\n",
        "    # 反方回應\n",
        "    response_B = client.chat.completions.create(\n",
        "        model=model_B,\n",
        "        messages=messages_B\n",
        "    )\n",
        "    reply_B = response_B.choices[0].message.content.strip()\n",
        "    print(\"反方：\", reply_B)\n",
        "    messages_B.append({\"role\": \"assistant\", \"content\": reply_B})\n",
        "    messages_A.append({\"role\": \"user\", \"content\": reply_B})\n",
        "\n",
        "    # 正方回應\n",
        "    response_A = client.chat.completions.create(\n",
        "        model=model_A,\n",
        "        messages=messages_A\n",
        "    )\n",
        "    reply_A = response_A.choices[0].message.content.strip()\n",
        "    print(\"正方：\", reply_A)\n",
        "    messages_A.append({\"role\": \"assistant\", \"content\": reply_A})\n",
        "    messages_B.append({\"role\": \"user\", \"content\": reply_A})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0dadJCuA_8H"
      },
      "source": [
        "而是我實際使用下來發現只有對話的話很難去總結辯論結果以及梳理邏輯，所以又自己另外加上總結產生函數來產生總結"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8im7ndLJKnq"
      },
      "source": [
        "這邊我將雙方辯論紀錄彙整為文字，以便後續進行總結"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gkLYFHlA_A3"
      },
      "outputs": [],
      "source": [
        "debate_text = []\n",
        "\n",
        "for msg in messages_A:\n",
        "    if msg[\"role\"] == \"assistant\":\n",
        "        debate_text.append(f\"正方：{msg['content']}\")\n",
        "for msg in messages_B:\n",
        "    if msg[\"role\"] == \"assistant\":\n",
        "        debate_text.append(f\"反方：{msg['content']}\")\n",
        "\n",
        "debate_summary_input = \"\\n\".join(debate_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lTVneG0BMmv"
      },
      "source": [
        "總結的「人設」:\n",
        "\n",
        "你是一位中立主持人，請針對以下雙方辯論做出總結，整理正反觀點，並提出你認為合理的結論，請用台灣習慣的中文來回應。\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd4GQyb7BNCl"
      },
      "outputs": [],
      "source": [
        "summary_prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一位中立主持人，請針對以下雙方辯論做出總結，整理正反觀點，並提出你認為合理的結論，請用台灣習慣的中文來回應。\"},\n",
        "    {\"role\": \"user\", \"content\": debate_summary_input}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqFNgG_9CdgH"
      },
      "source": [
        "由於我並未引入第三個模型進入作為總結機器人，而是使用了 mistral 模型做為總結機器人(也可視情況改為 ( gemma3:4b ) ，而這邊我不確定使用正方或者反方的機器人模型進行總結是否會有偏袒的行為產生，而這部分也是未來可以進行改進的(或者可以引入第三個模型進行總結)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX3IhlODAZFP",
        "outputId": "96f796e6-a6cd-44d8-dcc7-ed1d6004259b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 中立結論：\n",
            "\n",
            "總結：\n",
            "\n",
            "正方主要指出科學上存在先有蛋之法則、多元文化的尊重、創新思維、實際性和平衡力量與理性等優點，並強調應使用對等、尊重差異的方式來回顧文化問題，而反方主要強調在全球化中應讓不同文化問題各自定義標準、學習各種文化知識、提高信息溝通能力等重點。\n",
            "\n",
            "正方認為實踐至上，突破現有框架，利用理性去解決問題，是創造更輕鬆、穩健的未來方法，而反方則認為對現存價值產出的影響和創新思想的限制，在實際生活中首先做實務再去求義才能建立基石，進而發展。\n",
            "\n",
            "在斷舊建新時，反方強調只應保留一小部分基石，不必對過往產出情感。所以，根據這兩邊的論點，可以認為，在全球化中應該使用對等、尊重差異的方式來理解和回顧文化問題，並結合各種文化知識進行信息溝通，同時要注意先做實務再去求義，最後才能提升人類之力量。\n"
          ]
        }
      ],
      "source": [
        "response_summary = client.chat.completions.create(\n",
        "    model=model_B,  # or model_A (gemma3:4b)\n",
        "    messages=summary_prompt\n",
        ")\n",
        "\n",
        "summary = response_summary.choices[0].message.content.strip()\n",
        "\n",
        "print(\"\\n 中立結論：\\n\")\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owMkOMlNhdbe"
      },
      "source": [
        "### 4. 使用 gradio 套件打造 web app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pC1s368ihoNL",
        "outputId": "bdb11a23-cfb1-40aa-b3f6-7beec9dce5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqtUxj36hc3S"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUg_sz1ah2dk"
      },
      "source": [
        "對話機器人 app 設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60W4nBd0h15M"
      },
      "outputs": [],
      "source": [
        "title = \"正反方辯論機器人\"\n",
        "description = \"輸入一個議題，讓正方（gemma3:4b）與反方（mistral）進行辯論，各發言 10 回合，可選擇是否產出結論。\"\n",
        "\n",
        "model_A = \"gemma3:4b\"\n",
        "model_B = \"mistral\"\n",
        "\n",
        "system_A = \"你是一位邏輯嚴謹、辯才無礙的正方辯論者，擅長站在議題的支持角度提出實證與理性觀點。請使用有說服力的語氣，避免太過冗長，一次只發表一個觀點,儘量不要超過二十個字。請用台灣習慣的中文來回應。\"\n",
        "system_B = \"你是一位尖銳反思、觀點犀利的反方辯論者，擅長質疑主流觀點並從反面切入議題。請使用有力的批判性語氣，每次一句論述即可，避免過長,儘量不要超過二十個字。請用台灣習慣的中文來回應。\"\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=\"ollama\",\n",
        "    base_url=\"http://localhost:11434/v1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v56XiurHAO_j"
      },
      "source": [
        "系統訊息不會改變，用於 State 初始化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrMLfWtqdNqP"
      },
      "outputs": [],
      "source": [
        "initial_messages_A = [{\"role\": \"system\", \"content\": system_A}]\n",
        "initial_messages_B = [{\"role\": \"system\", \"content\": system_B}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOdMkfbBwCBV"
      },
      "outputs": [],
      "source": [
        "state = gr.State(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53zq7FBF5Uge"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "state_A = gr.State(initial_messages_A.copy())\n",
        "state_B = gr.State(initial_messages_B.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UmHZzAWHdNg"
      },
      "source": [
        "在這邊請了 ChatGPT 4o 協助進行了將前面的對話邏輯程式碼整理為 Gradio 的形式 ，並且我使用了另一段 system ( modle_B Mistral ) 指示讓模型中立總結這段辯論，這部分原本是 GPT 沒有進行新增的，而是我實際使用下來發現只有對話的話很難去總結辯論結果以及梳理邏輯，所以又自己另外加上總結產生函數來產生總結"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCsN_FzdiP_w"
      },
      "outputs": [],
      "source": [
        "def pipi(topic, rounds, with_summary):\n",
        "    messages_A = [{\"role\": \"system\", \"content\": system_A}, {\"role\": \"user\", \"content\": topic}]\n",
        "    messages_B = [{\"role\": \"system\", \"content\": system_B}]\n",
        "    log = [f\"📌 議題：{topic}\\n\"]\n",
        "\n",
        "    # 正方先講\n",
        "    res_A = client.chat.completions.create(model=model_A, messages=messages_A)\n",
        "    rep_A = res_A.choices[0].message.content.strip()\n",
        "    log.append(f\"正方：{rep_A}\\n\")\n",
        "    messages_A.append({\"role\": \"assistant\", \"content\": rep_A})\n",
        "    messages_B.append({\"role\": \"user\", \"content\": rep_A})\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        res_B = client.chat.completions.create(model=model_B, messages=messages_B)\n",
        "        rep_B = res_B.choices[0].message.content.strip()\n",
        "        log.append(f\"反方：{rep_B}\\n\")\n",
        "        messages_B.append({\"role\": \"assistant\", \"content\": rep_B})\n",
        "        messages_A.append({\"role\": \"user\", \"content\": rep_B})\n",
        "\n",
        "        res_A = client.chat.completions.create(model=model_A, messages=messages_A)\n",
        "        rep_A = res_A.choices[0].message.content.strip()\n",
        "        log.append(f\"正方：{rep_A}\\n\")\n",
        "        messages_A.append({\"role\": \"assistant\", \"content\": rep_A})\n",
        "        messages_B.append({\"role\": \"user\", \"content\": rep_A})\n",
        "\n",
        "    # 產出結論\n",
        "    if with_summary:\n",
        "        summary_prompt = \"請根據上面雙方的辯論，幫我中立總結雙方觀點，並提出你認為合理的結論。\"\n",
        "        summary_input = [{\"role\": \"system\", \"content\": \"你是一位中立主持人，請針對以下雙方辯論做出總結，整理正反觀點，並提出你認為合理的結論，請用台灣習慣的中文來回應\"},\n",
        "                         {\"role\": \"user\", \"content\": \"\\n\".join(log)}]\n",
        "        res_sum = client.chat.completions.create(model=model_B, messages=summary_input)\n",
        "        summary = res_sum.choices[0].message.content.strip()\n",
        "        log.append(f\"\\n結論：\\n{summary}\")\n",
        "\n",
        "    return \"\\n\".join(log)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKRhdD0KiUz-"
      },
      "outputs": [],
      "source": [
        "chatbot = gr.Textbox(label=\"辯論結果輸出區\", lines=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUV1hUi7D8-F"
      },
      "source": [
        "這邊我新增了 `rounds = gr.Slider` 來讓使用者可以自由選擇想要看到幾此辯論結果 ( 最多10次 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "collapsed": true,
        "id": "ZeOCHLvLxkab",
        "outputId": "0a427672-19cc-4278-bb6e-9868eaa001b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0dc7921638d7007104.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0dc7921638d7007104.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with gr.Blocks(title=title) as demo:\n",
        "    gr.Markdown(f\"## {title}\\n{description}\")\n",
        "\n",
        "    topic = gr.Textbox(label=\"請輸入辯論議題\", placeholder=\"例如：是否應該禁止 AI 創作藝術？\")\n",
        "    rounds = gr.Slider(1, 10, value=10, step=1, label=\"每方發言次數\")\n",
        "    summary = gr.Checkbox(label=\"是否產生總結\", value=True)\n",
        "\n",
        "    output = gr.Textbox(label=\"雙方辯論紀錄\", lines=30)\n",
        "\n",
        "    btn = gr.Button(\"開始辯論\")\n",
        "    btn.click(fn=pipi, inputs=[topic, rounds, summary], outputs=output)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}